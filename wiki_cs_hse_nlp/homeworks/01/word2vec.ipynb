{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "from nltk import ngrams\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, wordpunct_tokenize, word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from utils import get_distinct_words, read_corpus\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "<torch.cuda.device object at 0x7f5edeb2f220>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.device('cuda:1'))\n",
    "print(torch.cuda.device(0))\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 2\n",
    "ru_corpus_cp = read_corpus(\"ru_copy\")\n",
    "index_to_key, word_counter = get_distinct_words(ru_corpus_cp, min_count=min_count)\n",
    "index_to_key = [\"UNK\", \"PAD\"] + index_to_key\n",
    "key_to_index = {word: i for i, word in enumerate(index_to_key)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,\n",
       " [['кстати',\n",
       "   'как',\n",
       "   'неожиданно',\n",
       "   'кпрф',\n",
       "   'становиться',\n",
       "   'не',\n",
       "   'все',\n",
       "   'равный',\n",
       "   'на',\n",
       "   'судьба',\n",
       "   'фермер',\n",
       "   'именно',\n",
       "   'накануне',\n",
       "   'выборы'],\n",
       "  ['можно',\n",
       "   'и',\n",
       "   'по',\n",
       "   'другому',\n",
       "   'сказать',\n",
       "   'убогий',\n",
       "   'клоунада',\n",
       "   'кпрф',\n",
       "   'это',\n",
       "   'попытка',\n",
       "   'отвечать',\n",
       "   'на',\n",
       "   'запрос',\n",
       "   'молодой',\n",
       "   'поколение',\n",
       "   'левый',\n",
       "   'не',\n",
       "   'питать',\n",
       "   'иллюзия',\n",
       "   'по',\n",
       "   'повод',\n",
       "   'коммунистический',\n",
       "   'номенклатура',\n",
       "   'советский',\n",
       "   'образец',\n",
       "   'но',\n",
       "   'в',\n",
       "   'сила',\n",
       "   'свой',\n",
       "   'положение',\n",
       "   'под',\n",
       "   'давление',\n",
       "   'вызов',\n",
       "   'время',\n",
       "   'они',\n",
       "   'вынуждать',\n",
       "   'быть',\n",
       "   'меняться']])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ru_corpus_cp), ru_corpus_cp[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_matrix(sequences, key_to_index, UNK=\"UNK\", PAD=\"PAD\", max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = [x.split() for x in sequences]\n",
    "\n",
    "    max_sequence_len = max([len(x) for x in sequences])\n",
    "    if max_len is not None and max_sequence_len > max_len :\n",
    "        max_sequence_len = max_len\n",
    "\n",
    "    matrix = np.full((len(sequences), max_sequence_len), np.int32(key_to_index[PAD]))\n",
    "    for i, seq in enumerate(sequences):\n",
    "        row_ix = [key_to_index.get(word, key_to_index[UNK]) for word in seq[:max_sequence_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"ru_corpus_list\", \"rb\") as fp:\n",
    "    ru_corpus = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "309"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(len(ru_corpus_cp))\n",
    "# display(as_matrix(ru_corpus_cp, key_to_index, max_len=10))\n",
    "len(list(chain.from_iterable(ru_corpus_cp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def pad_text(text: list, window_size: int, pad: str):\n",
    "    appendix = [pad] * window_size\n",
    "\n",
    "    return appendix + text + appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEmbeddings(KeyedVectors):\n",
    "    def __init__(self, corpus, distinct_words=None, word_counter=None, vector_size=100, min_count=10):\n",
    "        super().__init__(vector_size=vector_size)\n",
    "        \n",
    "        self.index_to_key = distinct_words\n",
    "        self.word_counter = word_counter\n",
    "        if distinct_words is None or word_counter is None:\n",
    "            self.index_to_key, self.word_counter = get_distinct_words(corpus, min_count=min_count)\n",
    "    \n",
    "        self.key_to_index = {word: i for i, word in enumerate(self.index_to_key)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 36, 25,  9, 44, 41, 12, 30, 26, 30, 30, 19, 27, 23, 25, 18,  9,\n",
       "        6, 31, 44])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(50, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(u):\n",
    "    return torch.tensor([torch.exp(u_j) / torch.sum(torch.exp(u)) for u_j in u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts pass: 100\tloss: 11069.63671875\n",
      "texts pass: 200\tloss: 34508800.0\n",
      "texts pass: 300\tloss: 21.95401382446289\n",
      "texts pass: 400\tloss: 137.4266815185547\n",
      "texts pass: 500\tloss: 2649.57373046875\n",
      "texts pass: 600\tloss: 17090352.0\n",
      "texts pass: 700\tloss: 5576.408203125\n",
      "texts pass: 800\tloss: 47684120.0\n",
      "texts pass: 900\tloss: 823.6334838867188\n",
      "texts pass: 1000\tloss: 395664.78125\n",
      "texts pass: 1100\tloss: 93760.515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [15:51<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb Cell 13\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m             \u001b[39mprint\u001b[39m(loss)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mappend(epoch_loss)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m w2v \u001b[39m=\u001b[39m Word2Vec(ru_corpus) \u001b[39m# , min_count=2, window_size=3, n_epoches=10)\u001b[39;00m\n",
      "\u001b[1;32m/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m=\u001b[39m \u001b[39m0.0001\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW1, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW2], lr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(n_epoches)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW1\n",
      "\u001b[1;32m/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb Cell 13\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m window \u001b[39m=\u001b[39m [batch[i \u001b[39m+\u001b[39m j] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m i \u001b[39m+\u001b[39m j \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m i \u001b[39m+\u001b[39m j \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(batch) \u001b[39mand\u001b[39;00m i \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m batch[i \u001b[39m+\u001b[39m j] \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_to_key]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(window) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(center, window)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m     epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m     \u001b[39m# print(type(loss), loss)\u001b[39;00m\n",
      "\u001b[1;32m/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, center, window):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mone_hot_vector(center) \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW1      \u001b[39m# 1, vec_size = 1, vocab_size x vocab_size, vec_size\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m# print(\"h\", h.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m# u = h @ self.W2                                # 1, voc_size = 1, vec_size   x vec_size, vocab_size\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     k_neg \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_to_key), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_negative)\n",
      "\u001b[1;32m/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mone_hot_vector\u001b[39m(\u001b[39mself\u001b[39m, word: \u001b[39mstr\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     vector \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex_to_key), device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     vector[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_to_index[word]] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sibwa19/documents/online_courses/nlp_courses_implementation/wiki_cs_hse_nlp/homeworks/01/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m vector\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "class Word2Vec(BaseEmbeddings):\n",
    "    def __init__(self, corpus, distinct_words=None, vector_size=100, window_size=5,\n",
    "                 min_count=10, batch_size=None, n_negative=5, n_epoches=5):\n",
    "        super().__init__(corpus, vector_size=vector_size, distinct_words=distinct_words, min_count=min_count)\n",
    "\n",
    "        self.W1 = torch.randn((len(self.index_to_key), vector_size), device=device, requires_grad=True)  #, device=torch.cuda.device(0))  # vocab_size, vector_size\n",
    "        self.W2 = torch.randn((vector_size, len(self.index_to_key)), device=device, requires_grad=True)  #, device=torch.cuda.device(0))  # vector_size, vocab_size\n",
    "\n",
    "        self.corpus = corpus\n",
    "        self.window_size = window_size\n",
    "        self.batch_size = batch_size\n",
    "        if batch_size is None:\n",
    "            self.batch_size = np.max([len(text) for text in corpus])\n",
    "        self.n_negative = n_negative\n",
    "        self.alpha = 0.0001\n",
    "\n",
    "        self.optimizer = torch.optim.Adam([self.W1, self.W2], lr=self.alpha)\n",
    "        \n",
    "        self.train(n_epoches)\n",
    "        self.vectors = self.W1\n",
    "\n",
    "    def one_hot_vector(self, word: str):\n",
    "        vector = torch.zeros(len(self.index_to_key), device=device)\n",
    "        vector[self.key_to_index[word]] = 1\n",
    "\n",
    "        return vector\n",
    "    \n",
    "    def forward(self, center, window):\n",
    "        h = self.one_hot_vector(center) @ self.W1      # 1, vec_size = 1, vocab_size x vocab_size, vec_size\n",
    "\n",
    "        k_neg = np.random.choice(len(self.index_to_key), self.n_negative)\n",
    "        W2_neg = self.W2[:, k_neg]                     # vec_size, k_neg  \n",
    "        neg_sum = torch.sum(torch.exp(h @ W2_neg))     # sum(1, k_neg) = 1, vec_size x vec_size, k_neg\n",
    "\n",
    "\n",
    "        u_c = torch.sum(torch.tensor([h @ self.W2[:, self.key_to_index[context]] for context in window]))  # 1, vec_size x vec_size, 1\n",
    "\n",
    "        loss = -u_c + self.n_negative * neg_sum\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def train(self, n_epoches=5):\n",
    "        \"\"\"\n",
    "        trains self.center_W and self.context_W matrices\n",
    "        \"\"\"\n",
    "        self.losses = []\n",
    "        for epoch in tqdm(range(n_epoches)):\n",
    "            epoch_loss = 0\n",
    "            for t, text in enumerate(self.corpus):\n",
    "                if t % 100 == 0 and t != 0:\n",
    "                    print(f\"texts pass: {t}\\tloss: {loss}\")\n",
    "                for batch in chunks(text, self.batch_size):\n",
    "                    \n",
    "                    for j, center in enumerate(batch):\n",
    "                        if center in self.index_to_key:\n",
    "                            self.optimizer.zero_grad()\n",
    "\n",
    "                            window = [batch[i + j] for i in range(-self.window_size, self.window_size + 1, 1) if i + j >= 0 and i + j < len(batch) and i != 0 and batch[i + j] in self.index_to_key]\n",
    "\n",
    "                            if len(window) != 0:\n",
    "                                loss = self.forward(center, window)\n",
    "                                \n",
    "                                epoch_loss += loss\n",
    "\n",
    "                                loss.backward(retain_graph=True)\n",
    "                                self.optimizer.step()\n",
    "\n",
    "            print(loss)\n",
    "            self.losses.append(epoch_loss)\n",
    "\n",
    "w2v = Word2Vec(ru_corpus) # , min_count=2, window_size=3, n_epoches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306645"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ru_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
